{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "from align_utils import align_df, idr2_jdsx\n",
    "from plot_utils import clipped_heatmap, df_heatmap\n",
    "from red_likelihood import makeCArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figs = False\n",
    "if plot_figs:\n",
    "    import matplotlib as mpl\n",
    "    mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics of redundantly calibration + degenerately transformed datasets across JDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_time = 2458098.43869\n",
    "pol = 'ee'\n",
    "ndist = 'gaussian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('../rel_dfs', 'rel_df.{}.{}.md.pkl'.format(jd_time, pol)), \\\n",
    "          'rb') as f:\n",
    "    md = pickle.load(f)\n",
    "    \n",
    "vis_list = list(map(str, np.arange(md['no_unq_bls']*2).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idr2_df_path = '../aligned_red_deg.1.3826.ee.{}.pkl'.format(ndist)\n",
    "idr2_df = pd.read_pickle(idr2_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_integration = 53\n",
    "idr2_df_tint = idr2_df.xs(time_integration, level='time_int', drop_level=True).groupby(level=['freq']).median()\n",
    "idr2_df_tint = idr2_df_tint.apply(lambda row: pd.Series(makeCArray(row[vis_list].values.astype(float))), \\\n",
    "                                  axis=1)\n",
    "\n",
    "vis_abs_med = idr2_df_tint.abs().transpose()\n",
    "vmax = np.nanpercentile(vis_abs_med.values, 97)\n",
    "df_heatmap(vis_abs_med, xbase=50, ybase=5, vmax=vmax, vmin=0, figsize=(8, 5), \\\n",
    "           xlabel='Channel', ylabel='Redundant Baseline Group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idr2_df_tint = idr2_df.xs(time_integration, level='time_int', drop_level=True).groupby(level=['freq']).var()\n",
    "# turning into complex values\n",
    "idr2_df_tint = idr2_df_tint.apply(lambda row: pd.Series(makeCArray(row[vis_list].values.astype(float))), \\\n",
    "                                  axis=1)\n",
    "\n",
    "vis_var = np.sqrt(idr2_df_tint.abs()).transpose()\n",
    "vmax = np.nanpercentile(vis_var.values, 98)\n",
    "df_heatmap(vis_var, xbase=50, ybase=5, vmax=vmax, vmin=0, figsize=(8, 5), \\\n",
    "           xlabel='Channel', ylabel='Redundant Baseline Group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected baseline (14m EW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ew_bl_id = 2 # 14 m EW baselines\n",
    "\n",
    "Ntimes = idr2_df.index.get_level_values('time_int').unique().size\n",
    "Nfreqs = idr2_df.index.get_level_values('freq').unique().size\n",
    "\n",
    "grp = idr2_df.groupby(level=['freq', 'time_int'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.sqrt(np.square(grp.median()[str(ew_bl_id)]) + \\\n",
    "              np.square(grp.median()[str(ew_bl_id+1)])).values.reshape(Nfreqs, Ntimes)\n",
    "\n",
    "fig, ax = clipped_heatmap(arr.transpose(), 'Time Integration', 'Channel', \\\n",
    "                          vmin=0, figsize=(8, 5), sci_format=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.sqrt(np.square(grp.var()[str(ew_bl_id)]) + \\\n",
    "      np.square(grp.var()[str(ew_bl_id+1)])).values.reshape(Nfreqs, Ntimes)\n",
    "\n",
    "fig, ax = clipped_heatmap(arr.transpose(), 'Time Integration', 'Channel', \\\n",
    "                          vmin=0, clip_pctile=98, clip_rnd=100000, figsize=(8, 5), sci_format=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = idr2_df[['fun']].groupby(level=['freq', 'time_int'])\n",
    "logl_med = grp.median().values.reshape(Nfreqs, Ntimes)\n",
    "\n",
    "fig, ax = clipped_heatmap(logl_med.transpose(), 'Time Integration', 'Channel', \\\n",
    "                          vmin=0, clip_pctile=95, clip_rnd=10000, figsize=(8, 5), sci_format=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logl_std = grp.std().values.reshape(Nfreqs, Ntimes)\n",
    "\n",
    "fig, ax = clipped_heatmap(logl_std.transpose(), 'Time Integration', 'Channel', \\\n",
    "                          vmin=0, clip_pctile=95, clip_rnd=10000, figsize=(8, 5), sci_format=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flags from hera_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idr2_flags = np.load('../idr2_flags.npz')['arr_0']\n",
    "\n",
    "fig, ax = clipped_heatmap(idr2_flags.sum(axis=0).transpose(), 'Time Integration', 'Channel', \\\n",
    "                          vmin=0, clip_pctile=100, figsize=(8, 5))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_thresh = 3.3\n",
    "\n",
    "logl_mean = grp.mean().values.reshape(Nfreqs, Ntimes)\n",
    "idr2_flags = idr2_flags.astype(bool)\n",
    "logl_flags = np.empty_like(idr2_flags, dtype=bool)\n",
    "logls = np.empty_like(logl_flags, dtype=float)\n",
    "\n",
    "for i, jd in enumerate(idr2_jdsx):\n",
    "    logl_jd = idr2_df[['fun']].xs(jd, level='JD', drop_level=True).values.reshape(Nfreqs, Ntimes)\n",
    "    logl_flagi = np.logical_or(logl_jd > logl_mean + z_thresh*logl_std, logl_jd < logl_mean - z_thresh*logl_std)\n",
    "    logl_flags[i, ...] = logl_flagi\n",
    "    logls[i, ...] = logl_jd\n",
    "\n",
    "new_flags = np.logical_and(logl_flags, ~idr2_flags)\n",
    "print('{} potentially bad slices found that are not flagged through the '\\\n",
    "      'hera_cal pipeline, through std considerations'.format(np.sum(new_flags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_slices = np.where(new_flags)\n",
    "print(np.array(idr2_jdsx)[bad_slices[0]]) # JDs\n",
    "print(bad_slices[1] + 50) # Channels\n",
    "print(bad_slices[2]) # Time integrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logls[bad_slices], '\\n') # NLLs for bad slices\n",
    "print(logl_mean[bad_slices[1:]], '\\n') # med NLLs across JDs for each bad slice slice\n",
    "print(logl_std[bad_slices[1:]]*1e3, '\\n')  # NLLs std across JDs for each bad slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Z-scores & MAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction=1.4826\n",
    "\n",
    "meds = np.repeat(np.squeeze(idr2_df[['fun']].groupby(level=['freq', 'time_int']).\\\n",
    "                 median().values), len(idr2_jdsx))\n",
    "dev_from_med = idr2_df['fun'].values - meds\n",
    "mad = np.median(np.abs(dev_from_med).reshape(-1, len(idr2_jdsx)), axis=1)\n",
    "modz = dev_from_med/(correction*np.repeat(mad, len(idr2_jdsx)))\n",
    "modz = np.swapaxes(modz.reshape((len(idr2_jdsx), Ntimes, Nfreqs), order='F'), 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_slicesz = np.where(np.logical_and(np.abs(modz) > 11, ~idr2_flags))\n",
    "print('{} potentially bad slices found that are not flagged through the '\\\n",
    "      'hera_cal pipeline, through z-score considerations'.format(bad_slicesz[0].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modz[bad_slicesz], '\\n') # modified Z-score\n",
    "print(logls[bad_slicesz], '\\n') # NLLs for bad slices\n",
    "print(logl_mean[bad_slicesz[1:]], '\\n') # med NLLs across JDs for each bad slice slice\n",
    "print(logl_std[bad_slicesz[1:]]*1e3, '\\n')  # NLL std across JDs for each bad slice\n",
    "print(mad.reshape((Nfreqs, Ntimes))[bad_slicesz[1:]]*1e3)  # MAD for each bad slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_slices_t = tuple(np.append(bad_slices[i], bad_slicesz[i]) for i in range(len(bad_slices)))\n",
    "sort_index = np.argsort(bad_slices_t[0])\n",
    "bad_slices_t = tuple(b[sort_index] for b in bad_slices_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_print = np.empty((8, bad_slices_t[0].size))\n",
    "\n",
    "nice_print[0, :] = np.array(idr2_jdsx)[bad_slices_t[0]] # JDs\n",
    "nice_print[1, :] = bad_slices_t[1] + 50 # Channels\n",
    "nice_print[2, :] = bad_slices_t[2] # Time integrations\n",
    "nice_print[3, :] = logls[bad_slices_t] # NLLs\n",
    "nice_print[4, :] = logl_med[bad_slices_t[1:]] # med NLLs\n",
    "nice_print[5, :] = (logls[bad_slices_t] - logl_mean[bad_slices_t[1:]]) \\\n",
    "                   / logl_std[bad_slices_t[1:]] # Z-score\n",
    "nice_print[6, :] = modz[bad_slices_t] # Modified Z-score\n",
    "nice_print[7, :] = mad.reshape((Nfreqs, Ntimes))[bad_slices_t[1:]]*1e3 # MAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for LaTeX table formatting\n",
    "pp = nice_print.transpose()\n",
    "print('JD & Channel & Time & NLL & med NLL & Z-score & Z-score & MAD\\n')\n",
    "for i in range(bad_slices_t[0].size):\n",
    "    p = pp[i, :]\n",
    "    print('{} & {} & {} & {:.4f} & {:.4f} & {:.4f} & {:.4f} & {:.4f} \\\\\\\\'.\\\n",
    "          format(int(p[0]), int(p[1]), int(p[2]), p[3], p[4], p[5], p[6], p[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
